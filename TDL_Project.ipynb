{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49554fda-3f91-44d9-b8b1-53096eb82c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbaf692-a7a0-4f33-921b-4e2dccd7f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28971502-370d-4ba8-94ac-34bf2b857101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CharanSaiVaddi/negopt-sft-model\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"CharanSaiVaddi/negopt-sft-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e51c0c-aff2-4431-829a-53aaea856042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Helper to load an existing fine-tuned negative-prompt generator\n",
    "\n",
    "def load_neg_generator(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Loads a tokenizer and seq2seq model from `model_path`.\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizer\n",
    "    model = model\n",
    "    return tokenizer, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_negative(\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    max_length: int = 64,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates an optimized negative prompt for the given positive `prompt`.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    neg_prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return neg_prompt\n",
    "\n",
    "\n",
    "# Function to produce images with and without negative prompts\n",
    "\n",
    "def generate_images(\n",
    "    pos_prompt: str,\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    neg_generator: tuple,\n",
    "    guidance_scale: float = 7.5,\n",
    "    num_inference_steps: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - img_no_neg: image generated using only `pos_prompt`\n",
    "      - img_with_neg: image generated using an optimized negative prompt\n",
    "      - neg_prompt: the generated negative prompt string\n",
    "    \"\"\"\n",
    "    tokenizer, model = neg_generator\n",
    "    model.to(pipe.device)\n",
    "\n",
    "    # Generate image without negative prompt\n",
    "    img_no_neg = pipe(\n",
    "        prompt=pos_prompt,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps\n",
    "    ).images[0]\n",
    "\n",
    "    # Generate negative prompt and image\n",
    "    neg_prompt = generate_negative(pos_prompt, tokenizer, model)\n",
    "    img_with_neg = pipe(\n",
    "        prompt=pos_prompt,\n",
    "        negative_prompt=neg_prompt,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps\n",
    "    ).images[0]\n",
    "\n",
    "    return img_no_neg, img_with_neg, neg_prompt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Determine device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load Stable Diffusion pipeline\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the pre-fine-tuned neg-opt model\n",
    "    neg_generator = load_neg_generator(model, tokenizer)\n",
    "\n",
    "    # Example positive prompt\n",
    "    pos_prompt = \" Prompt: A Deer standing in the middle of a misty forest\"\n",
    "\n",
    "    # Generate images\n",
    "    img_no_neg, img_with_neg, neg_prompt = generate_images(\n",
    "        pos_prompt, pipe, neg_generator\n",
    "    )\n",
    "\n",
    "    # Save outputs\n",
    "    img_no_neg.save(\"output_no_negative.png\")\n",
    "    img_with_neg.save(\"output_with_negative.png\")\n",
    "    print(\"Generated negative prompt:\", neg_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b7c0f-8c7c-47a6-a33c-bad2a3081af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_no_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc25e62-5584-47e0-8586-e69669634f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d048d-342d-4097-a97f-8dd22b406661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#########################################\n",
    "# Configuration Settings\n",
    "#########################################\n",
    "class Config:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_timesteps = 50  # Number of timesteps in reverse diffusion.\n",
    "    # Quality threshold for the NIMA score (normalized to [0,1]).\n",
    "    quality_threshold = 0.5\n",
    "    output_dir = \"collected_data_large\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Curated list of nature-themed prompts.\n",
    "    prompts = [\n",
    "        \"a deer standing in the middle of a misty forest\"\n",
    "    ]\n",
    "    # Path to NIMA checkpoint (if you have one)\n",
    "    nima_checkpoint_path = \"nima_checkpoint.pth\"  # Update this if available\n",
    "\n",
    "#########################################\n",
    "# NIMA No-Reference Quality Assessment Model\n",
    "#########################################\n",
    "class NIMA(nn.Module):\n",
    "    def __init__(self, base_model: str = 'vgg16', num_classes: int = 10):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if base_model == 'vgg16':\n",
    "            # Use VGG16 features. (You can experiment with other backbones.)\n",
    "            vgg = models.vgg16(pretrained=True)\n",
    "            self.features = vgg.features\n",
    "            # Replace classifier with adaptive pooling and a fully-connected layer.\n",
    "            self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "            self.dropout = nn.Dropout(0.75)\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported base model\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)  # (B,512,1,1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # Raw logits\n",
    "\n",
    "    def predict_quality(self, x):\n",
    "        \"\"\"\n",
    "        Returns the expected quality score for input x.\n",
    "        The model outputs logits for scores 1..10 and we compute the expected value.\n",
    "        \"\"\"\n",
    "        logits = self.forward(x)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        # Scores 1 to 10.\n",
    "        scores = torch.arange(1, self.num_classes + 1, device=x.device, dtype=torch.float)\n",
    "        expected_quality = (prob * scores).sum(dim=1)\n",
    "        return expected_quality\n",
    "\n",
    "# Instantiate NIMA model and load checkpoint if available.\n",
    "def load_nima_model(config: Config) -> NIMA:\n",
    "    nima_model = NIMA(base_model='vgg16', num_classes=10).to(config.device)\n",
    "    if os.path.exists(config.nima_checkpoint_path):\n",
    "        checkpoint = torch.load(config.nima_checkpoint_path, map_location=config.device)\n",
    "        nima_model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded NIMA checkpoint.\")\n",
    "    else:\n",
    "        print(\"NIMA checkpoint not found; using base pretrained VGG16 features (not fine-tuned).\")\n",
    "    nima_model.eval()\n",
    "    return nima_model\n",
    "\n",
    "# Compute quality score using NIMA.\n",
    "def compute_nima_quality(image: Image.Image, nima_model: NIMA, config: Config) -> float:\n",
    "    # Define transforms to match what NIMA was trained on.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(config.device)\n",
    "    with torch.no_grad():\n",
    "        quality = nima_model.predict_quality(img_tensor)\n",
    "    # The expected quality score is between 1 and 10;\n",
    "    # normalize it to a 0-1 scale by subtracting 1 and dividing by 9.\n",
    "    quality_norm = (quality.item() - 1) / 9\n",
    "    return quality_norm\n",
    "\n",
    "#########################################\n",
    "# Diffusion Simulator (RGB)\n",
    "#########################################\n",
    "class DiffusionSimulator:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "    def reverse_diffusion(self, latent: torch.Tensor, prompt: str = \"\") -> Tuple[Image.Image, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Simulate the reverse diffusion process (optionally conditioned on a prompt).\n",
    "        Returns:\n",
    "            generated_image: Final generated image (PIL RGB).\n",
    "            noise_sequence: List of noise tensor predictions.\n",
    "        \"\"\"\n",
    "        noise_sequence = []\n",
    "        current_latent = latent.clone()\n",
    "        for t in range(self.config.num_timesteps):\n",
    "            # Simulated noise vector (random noise for demonstration).\n",
    "            predicted_noise = torch.randn_like(current_latent)\n",
    "            noise_sequence.append(predicted_noise.clone())\n",
    "            # Simplified latent update.\n",
    "            current_latent = current_latent - predicted_noise * 0.1\n",
    "        generated_image = self.tensor_to_image(current_latent)\n",
    "        return generated_image, noise_sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_image(tensor: torch.Tensor) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Converts a tensor in the range [-1,1] to a PIL RGB image.\n",
    "        Expects tensor shape (1, 3, H, W).\n",
    "        \"\"\"\n",
    "        tensor = tensor.clamp(-1, 1)\n",
    "        tensor = (tensor + 1) / 2  # Scale to [0,1]\n",
    "        tensor = tensor.squeeze(0)  # (3, H, W)\n",
    "        tensor = tensor.permute(1, 2, 0)  # (H, W, 3)\n",
    "        array = (tensor.cpu().numpy() * 255).astype(np.uint8)\n",
    "        return Image.fromarray(array, mode=\"RGB\")\n",
    "\n",
    "#########################################\n",
    "# Data Collection Pipeline with Self-Assessment\n",
    "#########################################\n",
    "class DataCollectionPipeline:\n",
    "    def __init__(self, diffusion_model: DiffusionSimulator, config: Config, nima_model: NIMA):\n",
    "        self.model = diffusion_model\n",
    "        self.config = config\n",
    "        self.nima_model = nima_model\n",
    "\n",
    "    # def collect_single_sample(self, latent: torch.Tensor, prompt: str) -> dict:\n",
    "    #     \"\"\"\n",
    "    #     Runs a single generation, computes quality using the NIMA model, and assigns a label.\n",
    "    #     \"\"\"\n",
    "    #     generated_image, noise_sequence = self.model.reverse_diffusion(latent, prompt=prompt)\n",
    "    #     quality_score = compute_nima_quality(generated_image, self.nima_model, self.config)\n",
    "    #     # Assign label: 1 if quality_score meets or exceeds threshold, else 0.\n",
    "    #     label = 1 if quality_score >= self.config.quality_threshold else 0\n",
    "\n",
    "    #     sample_data = {\n",
    "    #         \"prompt\": prompt,\n",
    "    #         \"noise_sequence\": [n.cpu().numpy().tolist() for n in noise_sequence],\n",
    "    #         \"generated_image\": np.array(generated_image),\n",
    "    #         \"quality_score\": quality_score,\n",
    "    #         \"label\": label\n",
    "    #     }\n",
    "    #     return sample_data\n",
    "\n",
    "    def collect_single_sample(self, latent: torch.Tensor, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Runs a single generation, computes quality using the NIMA model, and assigns a label.\n",
    "        \"\"\"\n",
    "        generated_image, noise_sequence = self.model.reverse_diffusion(latent, prompt=prompt)\n",
    "        quality_score = compute_nima_quality(generated_image, self.nima_model, self.config)\n",
    "        \n",
    "        # Assign label: 1 if quality_score meets or exceeds threshold, else 0.\n",
    "        label = 1 if quality_score >= self.config.quality_threshold else 0\n",
    "    \n",
    "        # Stack noise_sequence tensors into a single NumPy array of shape (num_timesteps, 3, 64, 64)\n",
    "        noise_sequence_np = torch.stack(noise_sequence).cpu().numpy()\n",
    "    \n",
    "        # Convert generated PIL image to NumPy array\n",
    "        generated_image_np = np.array(generated_image)\n",
    "    \n",
    "        sample_data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"noise_sequence\": noise_sequence_np,  # much more efficient than list of lists\n",
    "            \"generated_image\": generated_image_np,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"label\": label\n",
    "        }\n",
    "        return sample_data\n",
    "\n",
    "\n",
    "    # def run(self, num_samples: int) -> List[dict]:\n",
    "    #     \"\"\"\n",
    "    #     Generates a dataset by iterating over the prompt list (cycling if needed) and collecting samples.\n",
    "    #     \"\"\"\n",
    "    #     dataset = []\n",
    "    #     prompts = self.config.prompts\n",
    "    #     for sample_id in range(num_samples):\n",
    "    #         prompt = prompts[sample_id % len(prompts)]\n",
    "    #         # Generate an RGB latent tensor with dimensions (1, 3, 64, 64).\n",
    "    #         latent = torch.randn(1, 3, 64, 64, device=self.config.device)\n",
    "    #         sample_data = self.collect_single_sample(latent, prompt)\n",
    "    #         dataset.append(sample_data)\n",
    "    #         if (sample_id + 1) % 100 == 0:\n",
    "    #             print(f\"Collected {sample_id + 1} samples so far...\")\n",
    "    #     return dataset\n",
    "\n",
    "    def run_balanced(self, num_samples: int) -> Tuple[List[dict], float]:\n",
    "        \"\"\"\n",
    "        Generates a dataset and dynamically finds a threshold to balance label counts.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        quality_scores = []\n",
    "        prompts = self.config.prompts\n",
    "    \n",
    "        for sample_id in range(num_samples):\n",
    "            prompt = prompts[sample_id % len(prompts)]\n",
    "            latent = torch.randn(1, 3, 64, 64, device=self.config.device)\n",
    "            generated_image, noise_sequence = self.model.reverse_diffusion(latent, prompt=prompt)\n",
    "            quality_score = compute_nima_quality(generated_image, self.nima_model, self.config)\n",
    "    \n",
    "            sample = {\n",
    "                \"prompt\": prompt,\n",
    "                \"noise_sequence\": torch.stack(noise_sequence).cpu().numpy(),\n",
    "                \"generated_image\": np.array(generated_image),\n",
    "                \"quality_score\": quality_score  # temporarily store without label\n",
    "            }\n",
    "            samples.append(sample)\n",
    "            quality_scores.append(quality_score)\n",
    "    \n",
    "            if (sample_id + 1) % 100 == 0:\n",
    "                print(f\"Generated {sample_id + 1} samples...\")\n",
    "    \n",
    "        # Find median quality score as dynamic threshold for balance\n",
    "        scores_np = np.array(quality_scores)\n",
    "        dynamic_threshold = float(np.median(scores_np))\n",
    "        print(f\"Dynamic threshold selected: {dynamic_threshold:.4f}\")\n",
    "    \n",
    "        # Assign labels based on dynamic threshold\n",
    "        for sample in samples:\n",
    "            sample[\"label\"] = 1 if sample[\"quality_score\"] >= dynamic_threshold else 0\n",
    "    \n",
    "        # Optional: verify balance\n",
    "        labels = [s[\"label\"] for s in samples]\n",
    "        print(f\"Label counts: 0 ‚Üí {labels.count(0)}, 1 ‚Üí {labels.count(1)}\")\n",
    "    \n",
    "        return samples, dynamic_threshold\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Main Script\n",
    "# #########################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize configuration, diffusion simulator, and NIMA model.\n",
    "#     config = Config()\n",
    "#     diffusion_model = DiffusionSimulator(config)\n",
    "#     nima_model = load_nima_model(config)\n",
    "#     pipeline = DataCollectionPipeline(diffusion_model, config, nima_model)\n",
    "\n",
    "#     # Number of samples to generate.\n",
    "#     num_samples_to_generate = 1000\n",
    "#     dataset = pipeline.run(num_samples_to_generate)\n",
    "\n",
    "#     # Save the dataset to disk in NPZ format.\n",
    "#     dataset_path = os.path.join(config.output_dir, \"seq2classification_dataset.npz\")\n",
    "#     np.savez_compressed(dataset_path, dataset=dataset)\n",
    "#     print(f\"Dataset saved to {dataset_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    diffusion_model = DiffusionSimulator(config)\n",
    "    nima_model = load_nima_model(config)\n",
    "    pipeline = DataCollectionPipeline(diffusion_model, config, nima_model)\n",
    "\n",
    "    num_samples_to_generate = 1000\n",
    "    dataset, dynamic_thresh = pipeline.run_balanced(num_samples_to_generate)\n",
    "\n",
    "    # Save dataset (without quality_score if not needed)\n",
    "    for sample in dataset:\n",
    "        sample.pop(\"quality_score\", None)  # Optional: clean up if not needed\n",
    "\n",
    "    dataset_path = os.path.join(config.output_dir, \"seq2classification_dataset.npz\")\n",
    "    np.savez_compressed(dataset_path, dataset=dataset)\n",
    "    print(f\"Balanced dataset saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c000c-28e6-4720-a59b-d75cfd38aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have this:\n",
    "# from your_dataset_file import DiffusionSequenceDataset\n",
    "dataset = DiffusionSequenceDataset(\"collected_data_large/seq2classification_dataset.npz\")\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Count label distribution\n",
    "labels = [label for _, label in dataset]\n",
    "class_0 = labels.count(0)\n",
    "class_1 = labels.count(1)\n",
    "print(f\"Label distribution: [0]: {class_0}, [1]: {class_1}\")\n",
    "\n",
    "# ===============================\n",
    "# Simple CNN-RNN + Linear Classifier\n",
    "# ===============================\n",
    "# class CNN_RNN_Classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, kernel_size=3, padding=1),  # input channels = 3\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),  # 32x32\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),  # 16x16\n",
    "#             nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         )\n",
    "#         self.rnn = nn.GRU(input_size=64, hidden_size=128, batch_first=True)\n",
    "#         self.fc = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C, H, W = x.size()\n",
    "#         x = x.view(B * T, C, H, W)         # [B*T, C, H, W]\n",
    "#         x = self.cnn(x)                    # [B*T, 64, 1, 1]\n",
    "#         x = x.view(B, T, -1)               # [B, T, 64]\n",
    "#         _, h_n = self.rnn(x)               # [1, B, 128]\n",
    "#         h_n = h_n.squeeze(0)               # [B, 128]\n",
    "#         return self.fc(h_n)                # [B, 1] (raw logits)\n",
    "\n",
    "class ImprovedCNN_RNN_Classifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # More sophisticated CNN feature extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global pooling\n",
    "        )\n",
    "        \n",
    "        # Bidirectional GRU for better temporal modeling\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=128,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if dropout_rate > 0 and 2 > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*2, 128),  # *2 for bidirectional\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        x = self.cnn(x)  # [B*T, 128, 1, 1]\n",
    "        x = x.view(B, T, -1)  # [B, T, 128]\n",
    "        \n",
    "        # RNN sequence processing\n",
    "        output, _ = self.rnn(x)  # output shape: [B, T, 256*2]\n",
    "        \n",
    "        # Get final time step output\n",
    "        final_hidden = output[:, -1, :]  # [B, 256*2]\n",
    "        \n",
    "        # Classification\n",
    "        return self.fc(final_hidden)  # [B, 1]\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Move model to device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = BetterQualityMLP(input_dim=10).to(device)  # Make sure input_dim matches your data\n",
    "\n",
    "# # Loss & optimizer\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# # Add this before the training loop to inspect the data shape\n",
    "# for noise_seq, label in loader:\n",
    "#     print(f\"Noise sequence shape: {noise_seq.shape}\")\n",
    "#     print(f\"Label shape: {label.shape}\")\n",
    "#     break\n",
    "# # === Training Loop ===\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for noise_seq, label in loader:\n",
    "#         noise_seq = noise_seq.to(device).float()  # Shape: [B, input_dim]\n",
    "#         label = label.unsqueeze(1).float().to(device)  # Shape: [B, 1]\n",
    "\n",
    "#         pred = model(noise_seq)\n",
    "#         loss = loss_fn(pred, label)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# # ===============================\n",
    "# # Evaluation\n",
    "# # ===============================\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for noise_seq, label in loader:\n",
    "#         noise_seq = noise_seq.to(device)\n",
    "#         label = label.to(device).float()\n",
    "\n",
    "#         logits = model(noise_seq)\n",
    "#         probs = torch.sigmoid(logits)\n",
    "#         preds = (probs > 0.5).long().squeeze()\n",
    "\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.4\n",
    "BATCH_SIZE = 8  # Keep as is or adjust if memory allows\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImprovedCNN_RNN_Classifier(dropout_rate=DROPOUT_RATE).to(device)\n",
    "\n",
    "# Get class weights for balanced learning\n",
    "num_samples = len(dataset)\n",
    "class_0_weight = num_samples / (2 * class_0)\n",
    "class_1_weight = num_samples / (2 * class_1)\n",
    "class_weights = torch.tensor([class_0_weight, class_1_weight], device=device)\n",
    "\n",
    "# Loss & optimizer with weight decay for regularization\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_1_weight/class_0_weight], device=device))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Training loop with validation\n",
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "    \n",
    "    for noise_seq, label in loader:\n",
    "        noise_seq = noise_seq.to(device).float() / 255.0  # Normalize to [0,1]\n",
    "        label = label.unsqueeze(1).float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(noise_seq)\n",
    "        loss = loss_fn(pred, label)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        probs = torch.sigmoid(pred)\n",
    "        train_preds = (probs > 0.5).long().squeeze()\n",
    "        all_train_preds.extend(train_preds.cpu().numpy())\n",
    "        all_train_labels.extend(label.squeeze().cpu().numpy())\n",
    "    \n",
    "    # Training metrics\n",
    "    train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds)\n",
    "    \n",
    "    # Update learning rate based on loss\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    \n",
    "    # Validation phase - using training data as validation for this example\n",
    "    # In a real scenario, you would have a separate validation loader\n",
    "    model.eval()\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for noise_seq, label in loader:  # Using same loader for demonstration\n",
    "            noise_seq = noise_seq.to(device).float() / 255.0\n",
    "            label = label.to(device).float()\n",
    "            \n",
    "            logits = model(noise_seq)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long().squeeze()\n",
    "            \n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(label.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds)\n",
    "    print(f\"Validation - Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    # Save best model with F1 below 0.9\n",
    "    if val_f1 > best_val_f1 and val_f1 < 0.9:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"New best model saved with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for noise_seq, label in loader:\n",
    "        noise_seq = noise_seq.to(device).float() / 255.0\n",
    "        label = label.to(device).float()\n",
    "        \n",
    "        logits = model(noise_seq)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).long().squeeze()\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "final_acc = accuracy_score(all_labels, all_preds)\n",
    "final_f1 = f1_score(all_labels, all_preds)\n",
    "print(f\"Final Evaluation - Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da64e9-004b-421e-9eba-f847a72a1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('model.pkl','wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf0b14-1f40-44d2-88b0-887881f7f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedLatentCNN_RNN_Classifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Explicitly use 4 input channels for SD latents\n",
    "        self.cnn = nn.Sequential(\n",
    "            # First block - explicitly using 4 channel latent input\n",
    "            nn.Conv2d(4, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global pooling\n",
    "        )\n",
    "        \n",
    "        # Bidirectional GRU for better temporal modeling\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=128,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if dropout_rate > 0 and 2 > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*2, 128),  # *2 for bidirectional\n",
    "            nn.BatchNorm1d(128),    # Correctly use BatchNorm1d here\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process the batch dimension and sequence length appropriately\n",
    "        if len(x.shape) == 5:  # [B, T, C, H, W]\n",
    "            B, T, C, H, W = x.size()\n",
    "            # Reshape for CNN processing\n",
    "            x = x.reshape(B * T, C, H, W)\n",
    "        else:  # Handle the case where input might be [B, C, H, W]\n",
    "            B = x.size(0)\n",
    "            T = 1\n",
    "            x = x  # No reshape needed\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = self.cnn(x)  # [B*T, 128, 1, 1]\n",
    "        x = x.reshape(B, T, -1) if T > 1 else x.reshape(B, -1).unsqueeze(1)  # [B, T, 128]\n",
    "        \n",
    "        # RNN sequence processing\n",
    "        output, _ = self.rnn(x)  # output shape: [B, T, 256*2]\n",
    "        \n",
    "        # Get final time step output\n",
    "        final_hidden = output[:, -1, :]  # [B, 256*2]\n",
    "        \n",
    "        # Classification\n",
    "        return self.fc(final_hidden)  # [B, 1]\n",
    "\n",
    "# Modified classification function that handles the batched latent frames correctly\n",
    "def classify_with_latent_model(classifier, latent_frames, device):\n",
    "    \"\"\"\n",
    "    Classify latent noise frames with the CNN-RNN classifier\n",
    "    \n",
    "    Args:\n",
    "        classifier: The CNN-RNN classifier model\n",
    "        latent_frames: List of latent tensors or tensor with shape [T, 1, 4, 64, 64] or [1, T, 4, 64, 64]\n",
    "        device: Device to run classification on\n",
    "    \n",
    "    Returns:\n",
    "        prediction: Binary prediction (0 or 1)\n",
    "        probability: Confidence score (0 to 1)\n",
    "    \"\"\"\n",
    "    # Proper tensor preparation based on input format\n",
    "    if isinstance(latent_frames, list):\n",
    "        # Convert list of latents to proper tensor format\n",
    "        latent_batch = torch.stack(latent_frames, dim=0)  # [T, 1, 4, 64, 64]\n",
    "        # Take only the first dimension from the second axis (batch dimension)\n",
    "        latent_batch = latent_batch[:, 0:1, :, :, :]\n",
    "        # Reshape to [1, T, 4, 64, 64] (batch dim first)\n",
    "        latent_batch = latent_batch.permute(1, 0, 2, 3, 4)\n",
    "    else:\n",
    "        # Already a tensor, ensure correct shape\n",
    "        if latent_frames.dim() == 5:\n",
    "            if latent_frames.shape[0] > latent_frames.shape[1]:  # [T, B, C, H, W]\n",
    "                latent_batch = latent_frames.permute(1, 0, 2, 3, 4)  # -> [B, T, C, H, W]\n",
    "            else:\n",
    "                latent_batch = latent_frames  # Already [B, T, C, H, W]\n",
    "        else:\n",
    "            # Handle unexpected tensor shapes\n",
    "            raise ValueError(f\"Unexpected latent shape: {latent_frames.shape}\")\n",
    "    \n",
    "    # Debug information to check tensor shape and channels\n",
    "    print(f\"Debug - latent_batch shape before CNN: {latent_batch.shape}\")\n",
    "    \n",
    "    # Move to device and ensure float type\n",
    "    latent_batch = latent_batch.to(device).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = classifier(latent_batch)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        prediction = (probs > 0.5).long().item()\n",
    "    \n",
    "    return prediction, probs.item()\n",
    "\n",
    "# Main function to generate images with the improved classifier\n",
    "def generate_with_improved_classifier(prompt, model_ckpt_path=None, num_inference_steps=50, guidance_scale=7.5):\n",
    "    from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "    from PIL import Image\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load Stable Diffusion pipeline\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        use_safetensors=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Load DDIM scheduler\n",
    "    pipe.scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = pipe.scheduler.timesteps\n",
    "\n",
    "    # Get text embeddings with classifier-free guidance\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt,\n",
    "        device,\n",
    "        1,\n",
    "        do_classifier_free_guidance=True,\n",
    "    )\n",
    "\n",
    "    # Create the initial latent noise\n",
    "    # Use pipe.unet.config.in_channels instead of pipe.unet.in_channels to avoid deprecation warning\n",
    "    latents = torch.randn(\n",
    "        (1, pipe.unet.config.in_channels, 64, 64),\n",
    "        generator=None,\n",
    "        device=device,\n",
    "        dtype=pipe.unet.dtype\n",
    "    )\n",
    "    latents = latents * pipe.scheduler.init_noise_sigma\n",
    "\n",
    "    # Create and initialize the improved classifier\n",
    "    classifier = ImprovedLatentCNN_RNN_Classifier().to(device)\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    if model_ckpt_path:\n",
    "        try:\n",
    "            checkpoint = torch.load(model_ckpt_path, map_location=device)\n",
    "            classifier.load_state_dict(checkpoint)\n",
    "            print(\"‚úÖ Successfully loaded latent classifier checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading latent classifier checkpoint: {e}\")\n",
    "            print(\"‚ö† Continuing with untrained classifier\")\n",
    "\n",
    "    # For storing latent frames\n",
    "    latent_frames = []\n",
    "    # To keep snapshots of latents after each accepted step\n",
    "    accepted_latent_snapshots = []\n",
    "    rollback_limit = 10  # limit the number of rollbacks\n",
    "    step_results = []\n",
    "\n",
    "    print(f\"\\nüé® Starting diffusion with prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"üìà Number of timesteps: {len(timesteps)}\")\n",
    "\n",
    "    good_steps = 0   # counts accepted (good) steps\n",
    "    current_idx = 0  # index into the scheduler's timesteps\n",
    "\n",
    "    # We will run until all steps are complete\n",
    "    while current_idx < len(timesteps):\n",
    "        t = timesteps[current_idx]\n",
    "        # Save the current latent state (snapshot) so we can revert if necessary\n",
    "        accepted_latent_snapshots.append(latents.clone())\n",
    "\n",
    "        # Store the current latent for classification\n",
    "        latent_frames.append(latents.detach().clone())\n",
    "\n",
    "        # Expand the latents for classifier-free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if guidance_scale > 1.0 else latents\n",
    "        \n",
    "        # Predict the noise residual with the UNet\n",
    "        with torch.no_grad():\n",
    "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # Perform guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Only classify if we have enough frames for temporal context\n",
    "        prediction = 1  # Default to accepting the step\n",
    "        prob = 1.0\n",
    "        \n",
    "        if len(latent_frames) >= 3:\n",
    "            try:\n",
    "                # Use only the last 50 frames max to avoid memory issues\n",
    "                recent_frames = latent_frames[-50:]\n",
    "                \n",
    "                # Use the classify_with_latent_model function with our frames\n",
    "                prediction, prob = classify_with_latent_model(classifier, recent_frames, device)\n",
    "                step_results.append((current_idx, prediction, prob))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Classification error: {e}\")\n",
    "                # Continue with default prediction\n",
    "        \n",
    "        status = 'GOOD ‚úÖ' if prediction == 1 else 'FAIL ‚ùå'            \n",
    "        print(f\"  Step {current_idx+1}/{len(timesteps)}: {status} (confidence: {prob:.4f})\")\n",
    "\n",
    "        if prediction == 0:\n",
    "            if rollback_limit > 0 and current_idx > 0:\n",
    "                rollback_limit -= 1\n",
    "                # Revert to the previous accepted latent state\n",
    "                latents = accepted_latent_snapshots[current_idx - 1].clone()\n",
    "                latent_frames.pop()  # remove the current (failed) latent frame\n",
    "                print(f\"  üîÅ Rolling back from step {current_idx+1} (Rollback limit now: {rollback_limit})\")\n",
    "                # Do not increment current_idx so that we retry this timestep\n",
    "                continue\n",
    "            else:\n",
    "                print(\"  üö´ Rollback limit reached or no previous good step available; accepting FAIL step.\")\n",
    "        \n",
    "        # If the step is GOOD (or forced accepted), update latents normally:\n",
    "        latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        good_steps += 1\n",
    "        current_idx += 1\n",
    "\n",
    "    # After diffusion, decode the final latent to an image\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Convert latents to the VAE's dtype\n",
    "            latents_for_image = latents.to(pipe.vae.dtype)\n",
    "            latents_for_image = 1 / 0.18215 * latents_for_image  # Scale for VAE\n",
    "            decoded = pipe.vae.decode(latents_for_image).sample\n",
    "            image = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "            image = (image * 255).type(torch.uint8)\n",
    "            image = image.cpu().permute(0, 2, 3, 1)\n",
    "            final_image = Image.fromarray(image[0].numpy())\n",
    "            \n",
    "            # Save the image\n",
    "            final_image.save(\"generated_image.png\")\n",
    "            print(\"Image saved as 'generated_image.png'\")\n",
    "            \n",
    "            return final_image, latent_frames, step_results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating the final image: {e}\")\n",
    "        return None, latent_frames, step_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"A deer standing in middle of misty forest\"\n",
    "    model_ckpt_path = None  # No checkpoint needed for testing\n",
    "    \n",
    "    # Generate with improved latent noise classifier guidance\n",
    "    image, frames, results = generate_with_improved_classifier(\n",
    "        prompt, \n",
    "        model_ckpt_path,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35976cb5-9dc6-4a52-af4b-40810a3ab1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880aabb-7012-47ef-bc52-6da483e4fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the ImprovedLatentCNN_RNN_Classifier\n",
    "class ImprovedLatentCNN_RNN_Classifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Explicitly use 4 input channels for SD latents\n",
    "        self.cnn = nn.Sequential(\n",
    "            # First block - explicitly using 4 channel latent input\n",
    "            nn.Conv2d(4, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "\n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "\n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global pooling\n",
    "        )\n",
    "\n",
    "        # Bidirectional GRU for better temporal modeling\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=128,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if dropout_rate > 0 and 2 > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*2, 128),  # *2 for bidirectional\n",
    "            nn.BatchNorm1d(128),    # Correctly use BatchNorm1d here\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process the batch dimension and sequence length appropriately\n",
    "        if len(x.shape) == 5:  # [B, T, C, H, W]\n",
    "            B, T, C, H, W = x.size()\n",
    "            # Reshape for CNN processing\n",
    "            x = x.reshape(B * T, C, H, W)\n",
    "        else:  # Handle the case where input might be [B, C, H, W]\n",
    "            B = x.size(0)\n",
    "            T = 1\n",
    "            x = x  # No reshape needed\n",
    "\n",
    "        # CNN feature extraction\n",
    "        x = self.cnn(x)  # [B*T, 128, 1, 1]\n",
    "        x = x.reshape(B, T, -1) if T > 1 else x.reshape(B, -1).unsqueeze(1)  # [B, T, 128]\n",
    "\n",
    "        # RNN sequence processing\n",
    "        output, _ = self.rnn(x)  # output shape: [B, T, 256*2]\n",
    "\n",
    "        # Get final time step output\n",
    "        final_hidden = output[:, -1, :]  # [B, 256*2]\n",
    "\n",
    "        # Classification\n",
    "        return self.fc(final_hidden)  # [B, 1]\n",
    "\n",
    "\n",
    "def load_neg_generator():\n",
    "    \"\"\"\n",
    "    Load the negative prompt generator model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"CharanSaiVaddi/negopt-sft-model\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\"CharanSaiVaddi/negopt-sft-model\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading negative prompt generator: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_stable_diffusion():\n",
    "    \"\"\"\n",
    "    Load the Stable Diffusion pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load pipeline\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            use_safetensors=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Use DDIM scheduler\n",
    "        pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\", \n",
    "            subfolder=\"scheduler\"\n",
    "        )\n",
    "        \n",
    "        return pipe, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Stable Diffusion: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_classifier():\n",
    "    \"\"\"\n",
    "    Load the latent classifier model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        classifier = ImprovedLatentCNN_RNN_Classifier().to(device)\n",
    "        classifier.eval()\n",
    "        return classifier\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classifier: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_negative(\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    max_length: int = 64,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates an optimized negative prompt for the given positive `prompt`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        neg_prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return neg_prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating negative prompt: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def classify_with_latent_model(classifier, latent_frames, device):\n",
    "    \"\"\"\n",
    "    Classify latent noise frames with the CNN-RNN classifier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Proper tensor preparation based on input format\n",
    "        if isinstance(latent_frames, list):\n",
    "            # Convert list of latents to proper tensor format\n",
    "            latent_batch = torch.stack(latent_frames, dim=0)  # [T, 1, 4, 64, 64]\n",
    "            # Take only the first dimension from the second axis (batch dimension)\n",
    "            latent_batch = latent_batch[:, 0:1, :, :, :]\n",
    "            # Reshape to [1, T, 4, 64, 64] (batch dim first)\n",
    "            latent_batch = latent_batch.permute(1, 0, 2, 3, 4)\n",
    "        else:\n",
    "            # Already a tensor, ensure correct shape\n",
    "            if latent_frames.dim() == 5:\n",
    "                if latent_frames.shape[0] > latent_frames.shape[1]:  # [T, B, C, H, W]\n",
    "                    latent_batch = latent_frames.permute(1, 0, 2, 3, 4)  # -> [B, T, C, H, W]\n",
    "                else:\n",
    "                    latent_batch = latent_frames  # Already [B, T, C, H, W]\n",
    "            else:\n",
    "                # Handle unexpected tensor shapes\n",
    "                raise ValueError(f\"Unexpected latent shape: {latent_frames.shape}\")\n",
    "\n",
    "        # Move to device and ensure float type\n",
    "        latent_batch = latent_batch.to(device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(latent_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            prediction = (probs > 0.5).long().item()\n",
    "\n",
    "        return prediction, probs.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification: {e}\")\n",
    "        return 1, 1.0  # Default to accepting\n",
    "\n",
    "\n",
    "def advanced_image_generation(\n",
    "    prompt, \n",
    "    pipe, \n",
    "    device, \n",
    "    classifier, \n",
    "    negative_prompt=None, \n",
    "    use_classifier_guidance=True,\n",
    "    num_inference_steps=50, \n",
    "    guidance_scale=7.5,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate image with advanced techniques (negative prompts + classifier guidance)\n",
    "    \"\"\"\n",
    "    # Setup scheduler\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = pipe.scheduler.timesteps\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üé® Starting diffusion with prompt: \\\"{prompt}\\\"\")\n",
    "        if negative_prompt:\n",
    "            print(f\"üé® Using negative prompt: \\\"{negative_prompt}\\\"\")\n",
    "    \n",
    "    # Get text embeddings with classifier-free guidance\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt,\n",
    "        device,\n",
    "        1,\n",
    "        do_classifier_free_guidance=True,\n",
    "        negative_prompt=negative_prompt\n",
    "    )\n",
    "    \n",
    "    # Create the initial latent noise\n",
    "    latents = torch.randn(\n",
    "        (1, pipe.unet.config.in_channels, 64, 64),\n",
    "        generator=None,\n",
    "        device=device,\n",
    "        dtype=pipe.unet.dtype\n",
    "    )\n",
    "    latents = latents * pipe.scheduler.init_noise_sigma\n",
    "    \n",
    "    # Setup for latent frames and tracking\n",
    "    latent_frames = []\n",
    "    accepted_latent_snapshots = []\n",
    "    rollback_limit = 5  # limit the number of rollbacks\n",
    "    step_results = []\n",
    "    \n",
    "    good_steps = 0\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Step visualization data\n",
    "    step_indices = []\n",
    "    confidences = []\n",
    "    decisions = []\n",
    "    \n",
    "    # Generation loop\n",
    "    while current_idx < len(timesteps):\n",
    "        t = timesteps[current_idx]\n",
    "        # Progress updates\n",
    "        progress = (current_idx + 1) / len(timesteps)\n",
    "        if verbose:\n",
    "            print(f\"Progress: {progress*100:.1f}%\", end=\"\\r\")\n",
    "        \n",
    "        # Save the current latent state\n",
    "        accepted_latent_snapshots.append(latents.clone())\n",
    "        \n",
    "        # Store the current latent for classification\n",
    "        latent_frames.append(latents.detach().clone())\n",
    "        \n",
    "        # Expand the latents for classifier-free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if guidance_scale > 1.0 else latents\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # Perform guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        \n",
    "        # Classification and decision\n",
    "        prediction = 1  # Default to accepting\n",
    "        prob = 1.0\n",
    "        \n",
    "        if use_classifier_guidance and len(latent_frames) >= 3 and classifier is not None:\n",
    "            try:\n",
    "                # Use only the last 50 frames max\n",
    "                recent_frames = latent_frames[-50:]\n",
    "                prediction, prob = classify_with_latent_model(classifier, recent_frames, device)\n",
    "                step_results.append((current_idx, prediction, prob))\n",
    "                \n",
    "                # Store data for visualization\n",
    "                step_indices.append(current_idx)\n",
    "                confidences.append(prob)\n",
    "                decisions.append(prediction)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"‚ö†Ô∏è Classification error: {e}\")\n",
    "        \n",
    "        status = 'GOOD ‚úÖ' if prediction == 1 else 'FAIL ‚ùå'\n",
    "        if verbose:\n",
    "            print(f\"Step {current_idx+1}/{len(timesteps)}: {status} (confidence: {prob:.4f})\")\n",
    "        \n",
    "        # Handle rejected steps\n",
    "        if prediction == 0 and use_classifier_guidance:\n",
    "            if rollback_limit > 0 and current_idx > 0:\n",
    "                rollback_limit -= 1\n",
    "                # Revert to previous accepted state\n",
    "                latents = accepted_latent_snapshots[current_idx - 1].clone()\n",
    "                latent_frames.pop()  # remove failed frame\n",
    "                if verbose:\n",
    "                    print(f\"üîÅ Rolling back from step {current_idx+1} (Rollbacks left: {rollback_limit})\")\n",
    "                continue\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"üö´ Rollback limit reached; accepting anyway\")\n",
    "        \n",
    "        # Update latents\n",
    "        latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        good_steps += 1\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Decode the final latent to an image\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            latents_for_image = latents.to(pipe.vae.dtype)\n",
    "            latents_for_image = 1 / 0.18215 * latents_for_image\n",
    "            decoded = pipe.vae.decode(latents_for_image).sample\n",
    "            image = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "            image = (image * 255).type(torch.uint8)\n",
    "            image = image.cpu().permute(0, 2, 3, 1)\n",
    "            final_image = Image.fromarray(image[0].numpy())\n",
    "            \n",
    "            # Create and return statistics\n",
    "            stats = {\n",
    "                \"total_steps\": len(step_results),\n",
    "                \"accepted_steps\": sum(1 for _, pred, _ in step_results if pred == 1),\n",
    "                \"rejected_steps\": sum(1 for _, pred, _ in step_results if pred == 0),\n",
    "                \"step_details\": step_results,\n",
    "                \"confidence_values\": list(zip(step_indices, confidences))\n",
    "            }\n",
    "            \n",
    "            return final_image, stats\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå Error generating final image: {e}\")\n",
    "        return None, {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def visualize_results(image, stats, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the generation results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Generated Image')\n",
    "    \n",
    "    # Plot step decisions\n",
    "    if \"step_details\" in stats:\n",
    "        step_indices = [step for step, _, _ in stats[\"step_details\"]]\n",
    "        decisions = [pred for _, pred, _ in stats[\"step_details\"]]\n",
    "        confidences = [conf for _, _, conf in stats[\"step_details\"]]\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        colors = ['red' if d == 0 else 'green' for d in decisions]\n",
    "        plt.bar(step_indices, [1] * len(step_indices), color=colors)\n",
    "        plt.ylim(0, 1.2)\n",
    "        plt.title('Step Decisions (Green=Accept, Red=Reject)')\n",
    "        plt.xlabel('Diffusion Step')\n",
    "        \n",
    "        # Plot confidence scores\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(step_indices, confidences, marker='o', color='blue')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.title('Confidence Scores')\n",
    "        plt.xlabel('Diffusion Step')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "        \n",
    "        # Print statistics\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.axis('off')\n",
    "        stats_text = f\"\"\"\n",
    "        Generation Statistics:\n",
    "        - Total Steps: {stats.get('total_steps', 'N/A')}\n",
    "        - Accepted Steps: {stats.get('accepted_steps', 'N/A')}\n",
    "        - Rejected Steps: {stats.get('rejected_steps', 'N/A')}\n",
    "        \"\"\"\n",
    "        plt.text(0.1, 0.5, stats_text, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Results visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_image(image, file_path):\n",
    "    \"\"\"\n",
    "    Save the generated image to a file\n",
    "    \"\"\"\n",
    "    image.save(file_path)\n",
    "    print(f\"Image saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def generate_image_from_prompt(\n",
    "    prompt, \n",
    "    neg_prompt_option=\"auto\",\n",
    "    custom_neg_prompt=None,\n",
    "    use_classifier=True,\n",
    "    steps=50,\n",
    "    guidance=7.5,\n",
    "    visualize=True,\n",
    "    save_image_path=None,\n",
    "    save_viz_path=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to generate an image from a prompt\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    if verbose:\n",
    "        print(\"Loading models...\")\n",
    "    \n",
    "    neg_tokenizer, neg_model = load_neg_generator()\n",
    "    pipe, device = load_stable_diffusion()\n",
    "    classifier = load_classifier() if use_classifier else None\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(\"‚ùå Failed to load Stable Diffusion. Cannot generate image.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Handle negative prompt\n",
    "    negative_prompt = None\n",
    "    if neg_prompt_option == \"auto\":\n",
    "        if neg_tokenizer is not None and neg_model is not None:\n",
    "            if verbose:\n",
    "                print(\"Generating negative prompt...\")\n",
    "            negative_prompt = generate_negative(prompt, neg_tokenizer, neg_model)\n",
    "            if verbose:\n",
    "                print(f\"Generated negative prompt: {negative_prompt}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Negative prompt generator not available\")\n",
    "    elif neg_prompt_option == \"custom\" and custom_neg_prompt:\n",
    "        negative_prompt = custom_neg_prompt\n",
    "    \n",
    "    # Generate image\n",
    "    if verbose:\n",
    "        print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    \n",
    "    result_image, stats = advanced_image_generation(\n",
    "        prompt,\n",
    "        pipe,\n",
    "        device,\n",
    "        classifier,\n",
    "        negative_prompt,\n",
    "        use_classifier,\n",
    "        steps,\n",
    "        guidance,\n",
    "        verbose\n",
    "    )\n",
    "    \n",
    "    if result_image:\n",
    "        if verbose:\n",
    "            print(\"‚úÖ Image generation complete!\")\n",
    "        \n",
    "        # Save image if requested\n",
    "        if save_image_path:\n",
    "            save_image(result_image, save_image_path)\n",
    "        \n",
    "        # Visualize results if requested\n",
    "        if visualize:\n",
    "            visualize_results(result_image, stats, save_viz_path)\n",
    "            \n",
    "        return result_image, stats\n",
    "    else:\n",
    "        print(\"‚ùå Failed to generate image.\")\n",
    "        return None, stats\n",
    "\n",
    "\n",
    "# Example usage in a Python script or notebook\n",
    "if __name__ == \"__main__\":\n",
    "    # Example prompt\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    \n",
    "    # Generate image\n",
    "    image, stats = generate_image_from_prompt(\n",
    "        prompt=prompt,\n",
    "        neg_prompt_option=\"auto\",  # Options: \"none\", \"auto\", \"custom\"\n",
    "        custom_neg_prompt=\"low quality, blurry, distorted\",  # Used if neg_prompt_option=\"custom\"\n",
    "        use_classifier=True,\n",
    "        steps=50,\n",
    "        guidance=7.5,\n",
    "        visualize=True,\n",
    "        save_image_path=\"generated_image.png\",\n",
    "        save_viz_path=\"generation_results.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575705ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision scikit-learn scipy evaluate pytorch-fid torchmetrics transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b15d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from pytorch_fid import fid_score  # FID metric\n",
    "from torchmetrics.image.inception import InceptionScore  # Inception Score metric\n",
    "from torchmetrics.multimodal import CLIPScore  # CLIPScore metric\n",
    "import evaluate  # Hugging Face evaluate for BLEU/ROUGE\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from PIL import Image  # Ensure Image is defined\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Latent CNN-RNN Classifier Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "def eval_latent_classifier(y_true, y_pred_logits):\n",
    "    \"\"\"\n",
    "    y_true: list or np.array of 0/1 labels\n",
    "    y_pred_logits: list or np.array of raw logits from model\n",
    "    \"\"\"\n",
    "    y_prob = torch.sigmoid(torch.tensor(y_pred_logits)).numpy()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    # Classification report: precision, recall, F1\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n",
    "    # Calibration curve & Brier score\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "    print(f\"Brier score: {brier:.4f}\")\n",
    "    return {\n",
    "        \"classification_report\": classification_report(y_true, y_pred, output_dict=True),\n",
    "        \"roc_auc\": auc,\n",
    "        \"calibration\": (mean_pred.tolist(), frac_pos.tolist()),\n",
    "        \"brier\": brier,\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Negative-Prompt Generator Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "# load GPT-2 for perplexity\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# Fix padding token issue for GPT2 tokenizer\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "gpt2.eval()\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")      # BLEU metric\n",
    "rouge = evaluate.load(\"rouge\")    # ROUGE metric (requires rouge_score, nltk)\n",
    "clipscore = CLIPScore()\n",
    "\n",
    "# Move CLIPScore to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clipscore = clipscore.to(device)\n",
    "\n",
    "def perplexity(texts):\n",
    "    # Tokenize with padding now that pad_token is set\n",
    "    encodings = gpt2_tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2(**encodings, labels=encodings[\"input_ids\"])\n",
    "    # mean per-token loss ‚Üí perplexity\n",
    "    ppl = torch.exp(outputs.loss)\n",
    "    return ppl.tolist()\n",
    "\n",
    "def eval_negative_prompts(references, generations, images_for_clip):\n",
    "    \"\"\"\n",
    "    references: list of human-written negative prompts\n",
    "    generations: list of auto-generated negative prompts\n",
    "    images_for_clip: list of PIL images corresponding to positive prompts\n",
    "    \"\"\"\n",
    "    # BLEU & ROUGE\n",
    "    bleu_res = bleu.compute(predictions=generations, references=[[r] for r in references])\n",
    "    rouge_res = rouge.compute(predictions=generations, references=references)\n",
    "    # Perplexity (fluency)\n",
    "    ppl = perplexity(generations)\n",
    "    # Prepare images for CLIPScore: convert PIL to torch.Tensor (C,H,W) on same device\n",
    "    img_tensors = []\n",
    "    for img in images_for_clip:\n",
    "        arr = np.array(img)\n",
    "        tensor = torch.tensor(arr).permute(2,0,1).float() / 255.0\n",
    "        img_tensors.append(tensor.to(device))\n",
    "    # Embedding-based CLIPScore: measure how well images avoid unwanted content\n",
    "    clip_res_tensor = clipscore(images=img_tensors, text=generations)\n",
    "    # clip_res_tensor is a zero-dim tensor, convert to python float\n",
    "    clip_score_val = clip_res_tensor.item()\n",
    "    return {\n",
    "        \"bleu\": bleu_res,\n",
    "        \"rouge\": rouge_res,\n",
    "        \"perplexity\": np.mean(ppl),\n",
    "        \"clipscore\": clip_res[\"clip_score\"].item(),\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Image Generation Metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "def eval_generated_images(real_dir, fake_dir, prompts, fake_images, device=device):\n",
    "    \"\"\"\n",
    "    real_dir: path to folder of real images\n",
    "    fake_dir: path to folder where fake images are saved\n",
    "    prompts: list of text prompts\n",
    "    fake_images: list of PIL images generated\n",
    "    \"\"\"\n",
    "    # Save fake images to disk for FID\n",
    "    os.makedirs(fake_dir, exist_ok=True)\n",
    "    for i, img in enumerate(fake_images):\n",
    "        img.save(os.path.join(fake_dir, f\"{i:04d}.png\"))\n",
    "\n",
    "    # 3.1 FID\n",
    "    fid_value = fid_score.calculate_fid_given_paths([real_dir, fake_dir], batch_size=32, device=device)\n",
    "\n",
    "    # 3.2 Inception Score\n",
    "    is_metric = InceptionScore()\n",
    "    is_metric = is_metric.to(device)\n",
    "    fake_tensors = torch.stack([\n",
    "        torch.tensor(np.array(img)).permute(2,0,1)\n",
    "        for img in fake_images\n",
    "    ]).float() / 255.0\n",
    "    fake_tensors = fake_tensors.to(device)\n",
    "    is_res = is_metric(fake_tensors)\n",
    "\n",
    "    # 3.3 CLIPScore: convert PIL to torch.Tensor\n",
    "    img_tensors = [\n",
    "        torch.tensor(np.array(img)).permute(2,0,1).float()/255.0 for img in fake_images\n",
    "    ]\n",
    "    img_tensors = [t.to(device) for t in img_tensors]\n",
    "    clip_res = clipscore(images=img_tensors, text=prompts)\n",
    "\n",
    "    return {\n",
    "        \"FID\": fid_value,\n",
    "        \"InceptionScore\": {\"mean\": is_res[\"inception_score\"].item(), \"std\": is_res[\"inception_score_std\"].item()},\n",
    "        \"CLIPScore\": clip_res[\"clip_score\"].item(),\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Ablation & Statistical Significance\n",
    "# -----------------------------------------------------------------------------\n",
    "def ablation_ttest(metric_a, metric_b):\n",
    "    \"\"\"\n",
    "    Paired t-test between two metric arrays (e.g. FID with vs. without classifier)\n",
    "    \"\"\"\n",
    "    t_stat, p_val = stats.ttest_rel(metric_a, metric_b)\n",
    "    print(f\"Paired t-test: t={t_stat:.4f}, p={p_val:.4f}\")\n",
    "    return {\"t_stat\": t_stat, \"p_value\": p_val}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example usage\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Classifier\n",
    "    y_true = np.random.randint(0,2,100)\n",
    "    y_logits = np.random.randn(100)\n",
    "    cls_results = eval_latent_classifier(y_true, y_logits)\n",
    "\n",
    "    # 2) Negative prompts (dummy)\n",
    "    refs = [\"low quality\", \"blurry\", \"bad anatomy\"] * 10\n",
    "    gens = [\"blurry, low detail\"] * 30\n",
    "    neg_results = eval_negative_prompts(refs, gens, images_for_clip=[Image.new(\"RGB\",(64,64))]*30)\n",
    "\n",
    "    # 3) Image metrics (dummy directories & data)\n",
    "    real_dir = \"data/real\"\n",
    "    fake_dir = \"data/fake\"\n",
    "    prompts = [\"a cat on a sofa\"]*10\n",
    "    fake_images = [Image.new(\"RGB\",(256,256),color=(i*20, i*20, i*20)) for i in range(10)]\n",
    "    img_results = eval_generated_images(real_dir, fake_dir, prompts, fake_images)\n",
    "\n",
    "    # 4) Ablation: compare two FID runs\n",
    "    fid_a = np.random.randn(10)*5 + 50\n",
    "    fid_b = fid_a - (np.random.rand(10)*2)\n",
    "    ablation = ablation_ttest(fid_a, fid_b)\n",
    "\n",
    "    print(\"=== RESULTS ===\")\n",
    "    print(\"Classifier:\", cls_results)\n",
    "    print(\"NegPrompt:\", neg_results)\n",
    "    print(\"Image:\", img_results)\n",
    "    print(\"Ablation:\", ablation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683137f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f81da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss\n",
    "import pandas as pd\n",
    "\n",
    "# Dummy dataset class for latent sequences\\ n\n",
    "class LatentSequenceDataset(Dataset):\n",
    "    def __init__(self, latents, labels):\n",
    "        # latents: Tensor [N, T, C, H, W]\n",
    "        # labels: array [N]\n",
    "        self.latents = latents\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents[idx], self.labels[idx]\n",
    "\n",
    "# Simple Vanilla CNN (no RNN)\n",
    "class VanillaCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C, H, W] or [B, C, H, W]\n",
    "        if x.ndim == 5:\n",
    "            # average over time\n",
    "            x = x.mean(dim=1)  # [B, C, H, W]\n",
    "        out = self.cnn(x).view(x.size(0), -1)\n",
    "        return self.fc(out).squeeze(1)\n",
    "\n",
    "# Improved CNN-RNN classifier\n",
    "class ImprovedLatentCNN_RNN_Classifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.rnn = nn.GRU(128, 256, num_layers=2, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        feats = self.cnn(x).view(B, T, -1)\n",
    "        out, _ = self.rnn(feats)\n",
    "        final = out[:, -1]\n",
    "        return self.fc(final).squeeze(1)\n",
    "\n",
    "# Generate synthetic data\n",
    "N, T, C, H, W = 200, 10, 4, 64, 64\n",
    "latents = torch.randn(N, T, C, H, W)\n",
    "labels = torch.randint(0, 2, (N,)).numpy()\n",
    "\n",
    "# Prepare dataset and split\n",
    "dataset = LatentSequenceDataset(latents, labels)\n",
    "train_size = int(0.8 * N)\n",
    "val_size = N - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "# Helper to extract features for sklearn\n",
    "def extract_features(loader, model=None):\n",
    "    X, y = [], []\n",
    "    for lat, lbl in loader:\n",
    "        if model is None:\n",
    "            # flatten mean-pool\n",
    "            x = lat.mean(dim=1).view(lat.size(0), -1).numpy()\n",
    "        else:\n",
    "            # get CNN features\n",
    "            with torch.no_grad():\n",
    "                x_in = lat.mean(dim=1)\n",
    "                feat = model.cnn(x_in).view(lat.size(0), -1).numpy()\n",
    "            x = feat\n",
    "        X.append(x)\n",
    "        y.append(lbl.numpy())\n",
    "    return np.concatenate(X), np.concatenate(y)\n",
    "\n",
    "metrics = {'model': [], 'accuracy': [], 'roc_auc': [], 'brier_score': []}\n",
    "\n",
    "# 1) Logistic Regression\n",
    "X_train, y_train = extract_features(train_loader)\n",
    "X_val, y_val = extract_features(val_loader)\n",
    "lr = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
    "y_prob_lr = lr.predict_proba(X_val)[:,1]\n",
    "metrics['model'].append('Logistic Regression')\n",
    "metrics['accuracy'].append(accuracy_score(y_val, y_prob_lr>0.5))\n",
    "metrics['roc_auc'].append(roc_auc_score(y_val, y_prob_lr))\n",
    "metrics['brier_score'].append(brier_score_loss(y_val, y_prob_lr))\n",
    "\n",
    "# 2) SVM\n",
    "svm = SVC(probability=True).fit(X_train, y_train)\n",
    "y_prob_svm = svm.predict_proba(X_val)[:,1]\n",
    "metrics['model'].append('SVM')\n",
    "metrics['accuracy'].append(accuracy_score(y_val, y_prob_svm>0.5))\n",
    "metrics['roc_auc'].append(roc_auc_score(y_val, y_prob_svm))\n",
    "metrics['brier_score'].append(brier_score_loss(y_val, y_prob_svm))\n",
    "\n",
    "# 3) Vanilla CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vanilla = VanillaCNN().to(device)\n",
    "opt = torch.optim.Adam(vanilla.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(5):\n",
    "    vanilla.train()\n",
    "    for lat, lbl in train_loader:\n",
    "        lat, lbl = lat.to(device), lbl.float().to(device)\n",
    "        logits = vanilla(lat)\n",
    "        loss = loss_fn(logits, lbl)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "vanilla.eval()\n",
    "y_prob_vanilla, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for lat, lbl in val_loader:\n",
    "        lat = lat.to(device)\n",
    "        probs = torch.sigmoid(vanilla(lat)).cpu().numpy()\n",
    "        y_prob_vanilla.extend(probs)\n",
    "        y_true.extend(lbl.numpy())\n",
    "metrics['model'].append('Vanilla CNN')\n",
    "metrics['accuracy'].append(accuracy_score(y_true, np.array(y_prob_vanilla)>0.5))\n",
    "metrics['roc_auc'].append(roc_auc_score(y_true, y_prob_vanilla))\n",
    "metrics['brier_score'].append(brier_score_loss(y_true, y_prob_vanilla))\n",
    "\n",
    "# 4) Improved CNN-RNN\n",
    "improved = ImprovedLatentCNN_RNN_Classifier().to(device)\n",
    "opt2 = torch.optim.Adam(improved.parameters(), lr=1e-3)\n",
    "for epoch in range(5):\n",
    "    improved.train()\n",
    "    for lat, lbl in train_loader:\n",
    "        lat, lbl = lat.to(device), lbl.float().to(device)\n",
    "        logits = improved(lat)\n",
    "        loss = loss_fn(logits, lbl)\n",
    "        opt2.zero_grad(); loss.backward(); opt2.step()\n",
    "improved.eval()\n",
    "y_prob_imp = []\n",
    "with torch.no_grad():\n",
    "    for lat, lbl in val_loader:\n",
    "        lat = lat.to(device)\n",
    "        probs = torch.sigmoid(improved(lat)).cpu().numpy()\n",
    "        y_prob_imp.extend(probs)\n",
    "metrics['model'].append('Improved CNN-RNN')\n",
    "metrics['accuracy'].append(accuracy_score(y_true, np.array(y_prob_imp)>0.5))\n",
    "metrics['roc_auc'].append(roc_auc_score(y_true, y_prob_imp))\n",
    "metrics['brier_score'].append(brier_score_loss(y_true, y_prob_imp))\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(metrics)\n",
    "print(df)\n",
    "\n",
    "df.to_csv('classifier_benchmark.csv', index=False)\n",
    "print(\"Saved results to classifier_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb4ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
