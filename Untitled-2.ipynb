{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets peft trl bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Hugging Face token\n",
    "HF_TOKEN = \"\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Configuration parameters\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "LORA_OUTPUT_DIR = f\"{OUTPUT_DIR}/lora-adapters\"\n",
    "MERGED_OUTPUT_DIR = f\"{OUTPUT_DIR}/merged-model\"\n",
    "HF_LORA_REPO = \"CharanSaiVaddi/gemma-2-2b-it-gsm8k-lora\"\n",
    "HF_MERGED_REPO = \"CharanSaiVaddi/gemma-2-2b-it-gsm8k-merged\"\n",
    "\n",
    "# Training parameters\n",
    "MICRO_BATCH_SIZE = 1\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 2e-4\n",
    "TRAIN_STEPS = 1000\n",
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "MAX_SEQ_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the GSM8K dataset\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format GSM8K examples in instruction format.\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    # Extract the final numeric answer from the step-by-step solution\n",
    "    final_answer = answer.split(\"####\")[1].strip() if \"####\" in answer else answer.strip()\n",
    "    \n",
    "    # Format it for Gemma-2 instruction format\n",
    "    formatted_prompt = f\"<start_of_turn>user\\nSolve this math problem step by step: {question}<end_of_turn>\\n<start_of_turn>model\\n{answer}<end_of_turn>\"\n",
    "    \n",
    "    return {\"text\": formatted_prompt}\n",
    "\n",
    "processed_dataset = train_dataset.map(format_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__) # Make sure version is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes --force-reinstall # Ensure bitsandbytes is installed with the latest version\n",
    "import bitsandbytes as bnb # Import bitsandbytes\n",
    "\n",
    "# Configure quantization settings\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer padding token if needed\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bf16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    # eval_strategy=\"steps\", \n",
    "    # eval_steps=EVAL_STEPS,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Set up the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter model locally\n",
    "trainer.model.save_pretrained(LORA_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
    "\n",
    "# Push LoRA adapter to HF Hub\n",
    "model.push_to_hub(HF_LORA_REPO)\n",
    "tokenizer.push_to_hub(HF_LORA_REPO)\n",
    "\n",
    "# Create and save the merged model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "adapter_model = PeftModel.from_pretrained(base_model, LORA_OUTPUT_DIR)\n",
    "merged_model = adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model locally\n",
    "merged_model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "\n",
    "# Push the merged model to HF Hub\n",
    "merged_model.push_to_hub(HF_MERGED_REPO)\n",
    "tokenizer.push_to_hub(HF_MERGED_REPO)\n",
    "\n",
    "print(f\"Training complete! Models saved to:\")\n",
    "print(f\"LoRA adapters: {HF_LORA_REPO}\")\n",
    "print(f\"Merged model: {HF_MERGED_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference code to test the model\n",
    "def test_model(model_id, question):\n",
    "    \"\"\"Test the model with a sample question\"\"\"\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    prompt = f\"<start_of_turn>user\\nSolve this math problem step by step: {question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = test_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    return test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Uncomment to test your fine-tuned model\n",
    "sample_question = \"John has 5 apples. He buys 2 more apples and then gives 3 apples to his friend. How many apples does John have now?\"\n",
    "result = test_model(HF_MERGED_REPO, sample_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
